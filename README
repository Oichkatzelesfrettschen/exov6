NOTE: MIT have stopped maintaining the x86 version of xv6, and switched
their efforts to the RISC-V version
(https://github.com/mit-pdos/xv6-riscv.git)

This project experiments with decomposition of the xv6 operating system into an exokernel model. 
The project charter in [doc/charter.md](doc/charter.md) summarizes our goals and contributor guidelines. Philosophical technical debt notes are kept in [doc/ptd_log.md](doc/ptd_log.md).
The evolving project roadmap can be found in [doc/roadmap.md](doc/roadmap.md).


xv6 is a re-implementation of Dennis Ritchie's and Ken Thompson's Unix
Version 6 (v6).  xv6 loosely follows the structure and style of v6,
but is implemented for a modern x86-based multiprocessor using ANSI C.

The user-level shell supports built-in commands such as ``cd`` and ``exit``.  These
built-ins execute directly in the shell process even when used in command
lists, pipelines, or background jobs. Programs invoked by the shell are
searched using the ``PATH`` environment variable when present; if ``PATH``
is unset, the shell falls back to searching only ``/``.

Programs are expected to reside directly under `/`.  If a command
name does not include a slash, the shell automatically prepends `/`
when searching for the executable.  Other environment variables are
not yet supported.  Standard software can still be ported using the
POSIX wrappers described in ``doc/posix_user_guide.md``.

ACKNOWLEDGMENTS

xv6 is inspired by John Lions's Commentary on UNIX 6th Edition (Peer
to Peer Communications; ISBN: 1-57398-013-7; 1st edition (June 14,
2000)). See also https://pdos.csail.mit.edu/6.828/, which
provides pointers to on-line resources for v6.

xv6 borrows code from the following sources:
    JOS (asm.h, elf.h, mmu.h, bootasm.S, ide.c, console.c, and others)
    Plan 9 (entryother.S, mp.h, mp.c, lapic.c)
    FreeBSD (ioapic.c)
    NetBSD (console.c)

The following people have made contributions: Russ Cox (context switching,
locking), Cliff Frey (MP), Xiao Yu (MP), Nickolai Zeldovich, and Austin
Clements.

We are also grateful for the bug reports and patches contributed by Silas
Boyd-Wickizer, Anton Burtsev, Cody Cutler, Mike CAT, Tej Chajed, eyalz800,
Nelson Elhage, Saar Ettinger, Alice Ferrazzi, Nathaniel Filardo, Peter
Froehlich, Yakir Goaron,Shivam Handa, Bryan Henry, Jim Huang, Alexander
Kapshuk, Anders Kaseorg, kehao95, Wolfgang Keller, Eddie Kohler, Austin
Liew, Imbar Marinescu, Yandong Mao, Matan Shabtay, Hitoshi Mitake, Carmi
Merimovich, Mark Morrissey, mtasm, Joel Nider, Greg Price, Ayan Shafqat,
Eldar Sehayek, Yongming Shen, Cam Tenny, tyfkda, Rafael Ubal, Warren
Toomey, Stephen Tu, Pablo Ventura, Xi Wang, Keiichi Watanabe, Nicolas
Wolovick, wxdao, Grant Wu, Jindong Zhang, Icenowy Zheng, and Zou Chang Wei.

The code in the files that constitute xv6 is
Copyright 2006-2018 Frans Kaashoek, Robert Morris, and Russ Cox.

ERROR REPORTS

We don't process error reports (see note on top of this file).

BUILDING AND RUNNING XV6

Meson is the recommended build system and produces the full disk image
containing all user programs. Configure a build directory and compile
everything with::

    meson setup build && ninja -C build

``clang`` is used by default; change ``CC`` to select another compiler.
The helper scripts detect ``bison`` automatically and it is only required
when building the example parser.

This project relies on the preprocessor features ``__has_include`` and
``__has_attribute``.  Use a compiler supporting these macros (GCC 5 or
Clang 3.3 and newer).

Step-by-step build and run commands::

    CC=clang meson setup build
    ninja -C build qemu

The project also ships a minimal ``CMakeLists.txt`` for generating
``compile_commands.json`` or integrating with external CMake tooling. It
builds the kernel and a few demo programs but does not produce the full
set of user binaries.

When adding new user-space utilities place the sources under
``examples/demos`` and update ``meson.build`` accordingly. If you rely
on the CMake configuration add the program target there as well.  The
filesystem image is regenerated automatically whenever user programs are
built.

The QEMU PC simulator is installed automatically by ``setup.sh``.
Bochs is optional but also supported via ``ninja -C build bochs``.

A few headers used solely by the kernel have been moved under
``kernel/include``.  Files like ``spinlock.h`` and ``sleeplock.h``
reside here instead of ``include``.  The CMake build adds this
directory to the compiler's include path automatically.

PHOENIX_PROF
-----------
`phoenix_prof` is a small profiling helper built along with the rest of the system. It counts SIMD versus scalar operations and measures IPC and context switch latency. Build it via `ninja -C build phoenix_prof` or the matching CMake target. See [doc/profiling_metrics.md](doc/profiling_metrics.md) for further details.

BOCHS
-----
The file ``dot-bochsrc`` contains the default configuration used when
running ``ninja -C build bochs``.  Severity directives such as ``panic:``,
``error:``, ``info:`` and ``debug:`` accept per-facility overrides after the
global ``action`` keyword.  These overrides let you customize how Bochs
reacts to events from individual subsystems.  Example::

    panic: action=ask, cdrom=report

This reports panics generated by the CD-ROM device while prompting for
all other panic events.  Valid actions include ``fatal``, ``report``,
``ignore`` and ``ask``.

Before building user-mode programs you must generate ``libos.a``,
the user-space runtime library.  This typically happens as an
implicit prerequisite, but if you see errors about a missing
``libos.a`` run ``ninja -C build libos`` explicitly and then rebuild your
programs.
A helper script, `setup.sh`, installs cross-compilers, build dependencies,
QEMU packages, and additional development tools.  Run it once before
building to ensure all prerequisites are available.  From the top-level
directory simply execute::

    sudo ./setup.sh

When running without internet access place all required `.deb` files under
`scripts/offline_packages/` and use::

    sudo ./setup.sh --offline

The script installs each package from that directory with `dpkg -i`.

This script installs bare-metal cross compiler packages such as
``gcc-i386-elf``/``g++-i386-elf`` and ``gcc-x86-64-elf``/``g++-x86-64-elf``
along with the rest of the toolchain.

This only needs to be done after cloning the repository or when
dependencies change.

The script now installs only the packages required for building and running
the tests: cross-compilers, QEMU and basic Python tooling such as ``pytest``.
Heavy machine learning frameworks are intentionally omitted to keep the
environment small.  Formatting and linting tools like ``clang-format`` and
``clang-tidy`` can be run manually when needed.

For automated formatting and linting the repository provides a
``.pre-commit-config.yaml`` file. Install the hooks with ``pre-commit
install`` to run these checks automatically. The checks will run on every
commit thereafter.


TESTING
-------
Unit tests live under the ``tests/`` directory and can be executed with
``pytest -q``.  The helper binary ``user/exo_unit_test`` is built
automatically when needed. Ensure QEMU is installed if the tests require
it (``setup.sh`` installs QEMU automatically).
Several tests compile small standalone C programs with the host compiler.
Make sure your ``gcc`` or ``clang`` version supports the ``-std=c2x``
option or these builds will fail.

For a quick reference of these steps see ``CONTRIBUTING.md``.


To validate patches before emailing them, enable the
``sendemail-validate`` hook::

    cp .git/hooks/sendemail-validate.sample .git/hooks/sendemail-validate
    chmod +x .git/hooks/sendemail-validate

A sample ``sendemail-validate`` hook is provided as
``scripts/sendemail-validate.sample``.  It runs spell checking,
``checkpatch.pl`` and a quick build in a temporary worktree before
patches are emailed.  Enable it with::

    cp scripts/sendemail-validate.sample .git/hooks/sendemail-validate
    chmod +x .git/hooks/sendemail-validate

``clang-tidy`` is wrapped by ``scripts/run-clang-tidy.sh`` which
generates ``compile_commands.json`` on demand using
``scripts/gen_compile_commands.py`` if it doesn't already exist.  The
helper script configures a ``build`` directory with ``cmake`` and copies
``build/compile_commands.json`` to the top-level directory.  Pass
``--meson <path>`` to use ``meson setup`` instead of ``cmake``.

Experimental support for building with the C23 standard and for
cross-compiling a 64-bit version is available. Pass ``-DARCH=x86_64``
to ``cmake`` when configuring to enable 64-bit builds and adjust
``CSTD`` to use a different C standard if desired.


When building with ``ARCH=x86_64`` the resulting artifacts will be named
``kernel64``, ``fs64.img``, ``xv6-64.img`` and ``xv6memfs-64.img``.
Use these files when running QEMU or other emulators.

64-BIT BUILD REQUIREMENTS
-------------------------
Building a 64-bit kernel requires a cross-compiler capable of producing
``x86_64-elf`` binaries.  On most Linux systems this means installing
``x86_64-elf-gcc`` and the matching ``x86_64-elf`` binutils package.
If such packages are not available from your distribution you can build
them from source using the standard GNU build process.

Once a cross-compiler is installed, build the kernel with::

    cmake -S . -B build -G Ninja -DARCH=x86_64 \
        -DCMAKE_C_COMPILER=x86_64-elf-gcc && ninja -C build

The resulting ``kernel.img`` can be run under QEMU with::

    ninja -C build qemu

The 64-bit port currently differs from the 32-bit version in a few
ways.  It is less thoroughly tested and some user-space tests may not
pass.  The memory layout also differs slightly in order to support 64-bit
addresses.

In 64-bit mode the kernel lives in the higher half of the address
space.  ``memlayout.h`` defines ``KERNBASE64`` at
``0xffffffff80000000`` with devices mapped near the top of the canonical
range starting at ``DEVSPACE64``.  ``PHYSTOP64`` specifies the upper
limit of usable physical memory.  When building for 64-bit these values
are selected automatically via conditional compilation and replace the
32-bit constants.

CROSS-COMPILING
---------------
The ``setup.sh`` script installs cross-compilers for several
architectures such as ARMv7, AArch64, PowerPC and PowerPC64 (both
big-endian and little-endian).  Pass the ``ARCH`` value to ``cmake`` when
configuring.  The current build system implements ``ARCH=aarch64`` in
addition to ``x86`` and ``x86_64``.  A broader overview of these targets
is provided in ``doc/multi_architecture.md``.

Build and run an AArch64 image with::

    cmake -S . -B build -G Ninja -DARCH=aarch64 && ninja -C build
    ./qemu-aarch64.sh

Other cross-compilers are ready for experimentation.  Example commands
for architectures that do not yet have dedicated run scripts are::

    cmake -S . -B build-arm -G Ninja -DARCH=arm && ninja -C build-arm        # ARMv7
    qemu-system-arm -M virt -nographic -kernel kernel-arm

    cmake -S . -B build-ppc -G Ninja -DARCH=powerpc && ninja -C build-ppc    # 32-bit PowerPC
    qemu-system-ppc -M g3beige -nographic -kernel kernel-powerpc

    cmake -S . -B build-ppc64 -G Ninja -DARCH=ppc64 && ninja -C build-ppc64      # PowerPC64 big-endian
      qemu-system-ppc64 -M pseries -nographic -kernel kernel-ppc64

      cmake -S . -B build-ppc64le -G Ninja -DARCH=ppc64le && ninja -C build-ppc64le    # PowerPC64 little-endian
      qemu-system-ppc64 -M pseries -cpu POWER8 -nographic -kernel kernel-ppc64le

For more details on supported architectures, CPU feature detection and the
fallback order see ``doc/multi_architecture.md``.

When additional ``ARCH`` values are implemented these commands will
build and boot the corresponding images.


BOCHS
-----
Logging actions in ``dot-bochsrc`` can now be tailored per facility.  A
default action may be followed by overrides for specific devices.  For
example::

    panic: action=ask, cdrom=report
    error: action=report, cdrom=ignore
    info:  action=report
    debug: action=ignore, cdrom=report

The above configuration reports panics from the CD-ROM while leaving
other devices interactive, ignores non-critical CD-ROM errors and still
prints its debug messages.

CMAKE BUILD
-----------
A minimal CMake configuration is provided.  After running ``setup.sh``
``clang`` and ``cmake`` are available.  Configure a build directory and
compile with::

    cmake -S . -B build -G Ninja && ninja -C build

Meson can configure the project as well::

    meson setup build && ninja -C build

Both build systems accept the same compiler selection.  Use
``CC=<triplet-gcc>`` or ``-DCMAKE_C_COMPILER=<triplet-gcc>`` when
cross-compiling.  For example::

    CC=clang meson setup build
    ninja -C build

Optional flags control the spinlock implementation and debugging
facilities.  Set them with ``-D<FLAG>=ON`` when invoking CMake or in the
environment for Meson:

``CONFIG_SMP``
    Enable multi-core support. When OFF spinlocks compile to no-ops.
``USE_TICKET_LOCK``
    Use ticket-based spinlocks instead of the default.
``SPINLOCK_UNIPROCESSOR``
    Optimize locks for single CPU systems. Implies CONFIG_SMP=OFF.
``SPINLOCK_DEBUG``
    Enable additional lock sanity checks.
``USE_CAPNP``
    Build Cap'n Proto helpers and example programs.
``USE_SIMD``
    Enable optional SIMD implementations of math routines when supported by the
    target CPU.  Implementations exist for x87 and MMX floating point, SSE
    through AVX512 and FMA on x86, plus NEON and AltiVec on ARM and PowerPC.
    When an extension is unavailable the generic C routines are used instead.
    The dispatch layer prefers AVX, SSE2, MMX and x87 in that order on x86,
    NEON on ARM and AltiVec/VSX on PowerPC.  See ``doc/multi_architecture.md``
    for the full table.

Example CMake usage::

    cmake -S . -B build -DUSE_TICKET_LOCK=ON && ninja -C build
    cmake -S . -B build -DUSE_CAPNP=ON && ninja -C build
    cmake -S . -B build -DUSE_SIMD=ON && ninja -C build

To compile for specific instruction sets pass ``-DCPUFLAGS`` to ``cmake``.
This variable adds raw compiler flags like ``-mavx2`` or ``-mfpu=neon`` and is
honoured by both build systems.  The ``scripts/build_isa_variants.sh`` helper
builds a series of kernels using different flags (x87, MMX, SSE, AVX2, FMA,
AVX512 and more).  Leaving ``CPUFLAGS`` empty produces a baseline build that
relies on the runtime fallback described above.

Using Meson::

    USE_TICKET_LOCK=1 meson setup build && ninja -C build
    USE_CAPNP=1 meson setup build && ninja -C build
    USE_SIMD=1 meson setup build && ninja -C build

When invoking Meson the ``CPUFLAGS`` variable can be provided in the
environment to select a specific ISA, e.g. ``CPUFLAGS='-mavx2' meson setup build``.

To switch back to qspinlocks::

    cmake -S . -B build -DUSE_TICKET_LOCK=OFF && ninja -C build


Disable the option by setting ``OFF`` (or ``0`` for Meson).  The
default build uses qspinlocks.

GDB UTILITIES
-------------
Helper macros for inspecting x86 descriptor tables live under
``tools/gdbutil``.  Load them in GDB with::

    source tools/gdbutil

Use ``printdesc`` or ``printdescs`` to display a single descriptor or a
range of entries.

CODE FORMATTING
---------------
The project includes a ``.clang-format`` file configured for the LLVM
style with two-space indentation and the C23 language standard.  Before
submitting patches, run ``clang-format`` on any modified sources:

    clang-format -i file.c


EXO STREAMS
-----------
``struct exo_stream`` links scheduler modules together.  Register a stream
with ``exo_stream_register`` and call ``exo_stream_yield`` or
``exo_stream_halt`` to invoke the registered callbacks.  See the
``EXO_STREAM`` design note for details.  ``yield()`` invokes
``exo_stream_yield`` before scheduling another process.  When the scheduler
has no runnable tasks it calls ``exo_stream_halt`` rather than spinning; the
default implementation issues ``hlt`` if no stream is registered.

``streams_stop()`` tears down the active STREAMS pipeline and wakes the
scheduler once pending messages are flushed. ``streams_yield()`` saves the
current module state, marks the pipeline runnable and returns control to the
scheduler. Both helpers are invoked automatically when registered through
``exo_stream_register`` and the scheduler calls ``exo_stream_halt`` or
``exo_stream_yield``.

                USER DEMO
    : EXO_YIELD_TO AND STREAMS
      -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -The example
      program ``exo_stream_demo`` under ``engine
                / user / user
                /`` is a minimal illustration of switching contexts
                 with ``exo_yield_to`` and using the placeholder
                 STREAMS ``stop`` and ``yield`` calls.Build the system with
``cmake
            - S.- B build - G Ninja &&ninja - C build``; the resulting
``fs.img`` will contain the new ``exo_stream_demo`` binary.  Run it inside QEMU to observe the stub
functions printing messages that indicate the expected invocation order.

USER DEMO: TYPED CHANNELS
-------------------------
``typed_chan_demo`` showcases the ``CHAN_DECLARE`` macro that creates a
typed wrapper around the basic capability IPC functions.  Messages are
defined using Cap'n Proto schemas under ``proto/`` and automatically
serialized when sending.  Building with ``cmake -S . -B build -G Ninja && ninja -C build`` will compile
``typed_chan_demo`` along with ``typed_chan_send`` and
``typed_chan_recv`` under ``examples/demos``.  After ``fs.img`` is
generated, run ``typed_chan_demo`` inside QEMU to see a Cap'n Proto
message sent and received through the typed channel API.

USER DEMO: CHANNELS, DAG SCHEDULER AND SUPERVISOR
-------------------------------------------------
``chan_dag_supervisor_demo`` combines typed channels, the DAG scheduler and
the supervisor helper.  Building the repository with the standard command
compiles the demo and its helper ``typed_chan_recv``.  After booting with ``ninja -C build qemu-nox``
run::

    $ chan_dag_supervisor_demo

It spawns ``typed_chan_recv`` via ``driver_spawn``, sends a typed ping
message, and yields through the DAG stream before exiting.

USER DEMO: SIMPLE DAG
---------------------
``dag_demo`` constructs a small weighted DAG using the user-space
scheduler helpers.  Running the demo prints the order nodes are executed
according to their weights and dependencies.


USER DEMO: BEATTY SCHEDULER
--------------------------
``beatty_demo`` exercises the kernel's Beatty scheduler and prints the
resulting task order. Build with ``cmake -S . -B build -G Ninja && ninja -C build`` so the binary is placed in
``fs.img`` and run it inside QEMU:

    $ beatty_demo

USER DEMO: HELLO CHANNEL
------------------------
``hello_channel`` shows typed channels together with capability
borrowing. After the build completes, boot with ``ninja -C build qemu-nox`` and run:

    $ hello_channel

USER DEMO: AFFINE CHANNEL
-------------------------
``affine_channel_demo`` demonstrates the ``AFFINE_CHAN_DECLARE`` macro
which restricts a channel to a single send.  After building with ``cmake -S . -B build -G Ninja && ninja -C build`` and booting with ``ninja -C build qemu-nox`` run:

    $ affine_channel_demo

USER DEMO: FBDEV DRIVER
-----------------------
``fbdev_demo`` uses the minimal DDEKit layer to send a simple pattern to
the framebuffer.  Build the system and after booting run:

    $ fbdev_demo


The demonstration shows how Beatty's weights pick the family and how the
DAG scheduler respects node priorities when executing that family.

DRIVER SUPERVISOR
-----------------
``rcrs`` is a small supervisor that keeps user-space drivers running.
It parses ``drivers.conf`` where each non-empty line lists a command
and arguments to execute.  Comment lines beginning with ``#`` are
ignored.  When a driver process exits the supervisor automatically
restarts it.  The configuration syntax and available flags are
documented in ``doc/rcrs.md``. Add the desired commands to
``drivers.conf`` and copy the file into the file system image so ``init``
can launch ``rcrs`` early during boot.

Example ``drivers.conf``::

    kbdserv
    pingdriver --timeout=60
    fileserver


IPC DESIGN NOTE
---------------
See [doc/IPC.md](doc/IPC.md) for an overview of the fast-path call, capability badges,
and the proposed endpoint and typed channel features.

ENDPOINTS
---------
Endpoints are lightweight message channels inspired by Mach ports and
seL4 endpoints.  Each endpoint maintains an internal queue of
``zipc_msg_t`` values.  The ``endpoint_send`` and ``endpoint_recv``
system calls enqueue and dequeue messages, with receivers sleeping until
a message is available.  This allows asynchronous waiting similar to the
channel primitives in those microkernels.

NUMA-AWARE ALLOCATOR
--------------------
The page allocator now exposes capability primitives. ``exo_alloc_page``
returns a capability describing a physical page while ``exo_unbind_page``
releases it. Each NUMA node maintains its own free list and a process
may request a preferred node with ``set_numa_node``. Allocation first
tries the preferred node before falling back to the others.



See doc/phoenixkernel.md for a design overview of the Phoenix exokernel
and step-by-step examples of capability allocation, typed channels and
driver management.  The document also outlines the planned BSD and SVR4
compatibility layers built on top of the libOS.
For build instructions and an overview of how capabilities provide POSIX semantics see [doc/posix_user_guide.md](doc/posix_user_guide.md).  The implementation status of individual wrappers is tracked in [doc/posix_progress.md](doc/posix_progress.md).
Additional notes on the current research direction can be found in
`doc/track_technical.md`, `doc/track_philosophy.md` and
`doc/track_hybrid.md`. These summaries reference the project charter
and provide a short overview of the exposed APIs and guiding
philosophy.

Additional documentation lives under the `doc/` directory.  The
[technical track](doc/track_technical.md) summarizes the kernel API and
libOS usage, while the [philosophy track](doc/track_philosophy.md)
explains the broader goals without delving into implementation.  A
brief [hybrid track](doc/track_hybrid.md) ties the two together.  All
three reference the Phoenix kernel charter for further context.

Example supervisor usage to manage drivers:

```
$ supervisor start blk_driver
... later ...
$ supervisor restart blk_driver
```

SWIFT EXAMPLE
-------------
A small Swift demo is available in the ``swift`` directory. The script
``swift/build_swift.sh`` compiles and runs ``hello.swift``.
Execute it from the repository root:

```
$ ./swift/build_swift.sh
```

It should print ``Hello, world!``.

FUZZING
-------
The repository includes a simple Swift libFuzzer harness in ``fuzz.swift``.
The helper script ``scripts/swift_fuzz.py`` compiles the harness and forwards
any arguments to the resulting fuzzer executable. A quick smoke test can be
run as follows:

```
$ python3 scripts/swift_fuzz.py -runs=1
```

Additional libFuzzer flags may be provided after the script name.

STREAMS STACK
-------------
The repository includes a detailed simulation of STREAMS modules.
The `streams_stack.dot` file describes the expanded module layout. Generate an updated image with Graphviz:

```
$ dot -Tpng streams_stack.dot -o stack.png
```

The resulting `stack.png` illustrates the module chain.

Run the latency harness with Python.  The script will benchmark several
module combinations and print the average, minimum and maximum latency
for each:

```
$ python3 scripts/simulate.py
xor: avg 0.50 us (min 0.40, max 0.60)
upper+xor: avg 0.70 us (min 0.60, max 0.80)
reverse+upper+xor: avg 0.90 us (min 0.80, max 1.00)
```

Further work items related to the STREAMS prototype are tracked in
`TODO_STREAMS.md`.

STREAMS FLOW CONTROL PROCFS
---------------------------
The adaptive PID loop used by the STREAMS prototype reads its tuning
constants from files under ``/proc/streams/fc/``. Three entries hold
the numeric values::

    /proc/streams/fc/Kp
    /proc/streams/fc/Ki
    /proc/streams/fc/Kd

Missing or malformed files fall back to built-in defaults. Writing a
new value to one of these files immediately updates the active
constant. The ``flow_pid.py`` helpers ``set_kp()``, ``set_ki()`` and
``set_kd()`` provide a convenient API for adjusting the parameters.

For testing the base directory can be overridden with the environment
variable ``FLOW_PID_DIR``.  A short walkthrough of the interface along
with a Python example is provided in
``doc/streams_flow_control.md``.

STREAMS LOGGING
---------------
Python STREAMS modules log messages using ``strlog_json()`` from
``streams_log.py``.  The helper outputs a JSON object containing the
timestamp, log level and message.  Extra keyword arguments become
fields in the record.  ``strlog()`` continues to print plain text for
older modules.

Example::

    from streams_log import strlog_json
    strlog_json("info", "module loaded", name="xor_encrypt")

AUTOMATED SETUP WITH CODEX
--------------------------
An optional workflow installs the Codex CLI and executes `setup.sh` automatically. A sample systemd unit file is provided at `scripts/codex-setup.service`. Enable it to heal and run the setup script before networking finishes starting:

    sudo cp scripts/codex-setup.service /etc/systemd/system/
    sudo systemctl daemon-reload
    sudo systemctl enable codex-setup.service

A minimal dev container configuration lives under `.devcontainer`. Opening the project in a Codespace installs Codex and runs `codex -q 'doctor setup.sh'` during initialization.


FORMAL MODELS
-------------
Coq proofs live under the `formal/` directory. TLA+ models can be found
under `specs/tla/`. Run `make -C formal/coq` to type-check the Coq
proofs and `tlc specs/tla/ExoCap.tla` to explore the TLA+ models. See
`doc/formal_models.md` for details on extending the models.
